{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb24ef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from pathlib import Path\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from transformers import LayoutLMv3FeatureExtractor, LayoutLMv3TokenizerFast, LayoutLMv3Processor, LayoutLMv3ForSequenceClassification\n",
    "from torchmetrics import Accuracy\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import json\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from typing import List\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import easyocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e9a8f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['email', 'resume', 'scientific_publication']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOCUMENT_CLASSES = ['email', 'resume', 'scientific_publication']\n",
    "DOCUMENT_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb965f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import nn\n",
    "\n",
    "# class ModelModule(nn.Module):\n",
    "class ModelModule(pl.LightningModule):\n",
    "    def __init__(self, n_classes:int):\n",
    "        super().__init__()\n",
    "        self.model = LayoutLMv3ForSequenceClassification.from_pretrained(\n",
    "            \"microsoft/layoutlmv3-base\", \n",
    "            num_labels=n_classes\n",
    "        )\n",
    "        self.model.config.id2label = {k: v for k, v in enumerate(DOCUMENT_CLASSES)}\n",
    "        self.model.config.label2id = {v: k for k, v in enumerate(DOCUMENT_CLASSES)}\n",
    "        self.train_accuracy = Accuracy(task=\"multiclass\", num_classes=n_classes)\n",
    "        self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, bbox, pixel_values, labels=None):\n",
    "        return self.model(\n",
    "            input_ids, \n",
    "            attention_mask=attention_mask,\n",
    "            bbox=bbox,\n",
    "            pixel_values=pixel_values,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        bbox = batch[\"bbox\"]\n",
    "        pixel_values = batch[\"pixel_values\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        output = self(input_ids, attention_mask, bbox, pixel_values, labels)\n",
    "        self.log(\"train_loss\", output.loss)\n",
    "        self.log(\"train_acc\", self.train_accuracy(output.logits, labels), on_step=True, on_epoch=True)\n",
    "        return output.loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        bbox = batch[\"bbox\"]\n",
    "        pixel_values = batch[\"pixel_values\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        output = self(input_ids, attention_mask, bbox, pixel_values, labels)\n",
    "        self.log(\"val_loss\", output.loss)\n",
    "        self.log(\"val_acc\", self.val_accuracy(output.logits, labels), on_step=False, on_epoch=True)\n",
    "        return output.loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.00001) #1e-5\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a536e356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForSequenceClassification were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Tahir iltaf\\.conda\\envs\\layoutlmv3_env\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:751: UserWarning: You passed `Trainer(accelerator='cpu', precision=16)` but native AMP is not supported on CPU. Using `precision='bf16'` instead.\n",
      "  rank_zero_warn(\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model_module = ModelModule(len(DOCUMENT_CLASSES))\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filename=\"{epoch}-{step}-{val_loss:.4f}\", save_last=True, save_top_k=3, monitor=\"val_loss\", mode=\"min\"\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    precision=16,\n",
    "    devices=1,\n",
    "    max_epochs=10,\n",
    "    callbacks=[\n",
    "        model_checkpoint\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09f91673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForSequenceClassification were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_path  = \"G:/programming/Packagex_assignment/LayoutLMv3/models/version_4/checkpoints/last.ckpt\"\n",
    "trained_model = ModelModule.load_from_checkpoint(\n",
    "    checkpoint_path = model_path, \n",
    "    n_classes=len(DOCUMENT_CLASSES), \n",
    "    local_files_only=True\n",
    ")\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = trained_model.model.eval().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2da4ea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_bounding_box(box: List[int], width_scale : float = 1.0, height_scale : float = 1.0) -> List[int]:\n",
    "    return [\n",
    "        int(box[0] * width_scale),\n",
    "        int(box[1] * height_scale),\n",
    "        int(box[2] * width_scale),\n",
    "        int(box[3] * height_scale)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e93195f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_document_image(\n",
    "    image_path: Path, \n",
    "    model: LayoutLMv3ForSequenceClassification, \n",
    "    processor: LayoutLMv3Processor):\n",
    "\n",
    "    json_path = image_path.with_suffix(\".json\")\n",
    "    with json_path.open(\"r\") as f:\n",
    "        ocr_result = json.load(f)\n",
    "\n",
    "        with Image.open(image_path).convert(\"RGB\") as image:\n",
    "\n",
    "            width, height = image.size\n",
    "            width_scale = 1000 / width\n",
    "            height_scale = 1000 / height\n",
    "    \n",
    "            words = []\n",
    "            boxes = []\n",
    "            for row in ocr_result:\n",
    "                boxes.append(\n",
    "                    scale_bounding_box(\n",
    "                        row[\"bounding_box\"], \n",
    "                        width_scale, \n",
    "                        height_scale\n",
    "                    )\n",
    "                )\n",
    "                words.append(row[\"word\"])\n",
    "    \n",
    "            encoding = processor(\n",
    "                image, \n",
    "                words,\n",
    "                boxes=boxes,\n",
    "                max_length=512,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output = model(\n",
    "            input_ids=encoding[\"input_ids\"].to(DEVICE),\n",
    "            attention_mask=encoding[\"attention_mask\"].to(DEVICE),\n",
    "            bbox=encoding[\"bbox\"].to(DEVICE),\n",
    "            pixel_values=encoding[\"pixel_values\"].to(DEVICE)\n",
    "        )\n",
    "\n",
    "    predicted_class = output.logits.argmax()\n",
    "    return model.config.id2label[predicted_class.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8097755a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA not available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "reader = easyocr.Reader(['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e7340ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bounding_box(bbox_data):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for x, y in bbox_data:\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    \n",
    "    left = int(min(xs))\n",
    "    top = int(min(ys))\n",
    "    right = int(max(xs))\n",
    "    bottom = int(max(ys))\n",
    "\n",
    "    return [left, top, right, bottom]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d3494e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_document_image2(\n",
    "    image_path: Path, \n",
    "    model: LayoutLMv3ForSequenceClassification, \n",
    "    processor: LayoutLMv3Processor):\n",
    "\n",
    "    json_path = image_path.with_suffix(\".json\")\n",
    "    with json_path.open(\"r\") as f:\n",
    "        ocr_result = json.load(f)\n",
    "\n",
    "        with Image.open(image_path).convert(\"RGB\") as image:\n",
    "\n",
    "            width, height = image.size\n",
    "            width_scale = 1000 / width\n",
    "            height_scale = 1000 / height\n",
    "            \n",
    "            words = []\n",
    "            boxes = []\n",
    "            ocr_result = reader.readtext(str(image_path))    \n",
    "            for bbox, word, confidence in ocr_result:\n",
    "                boxes.append(scale_bounding_box(create_bounding_box(bbox), width_scale, height_scale))\n",
    "                words.append(word)\n",
    "    \n",
    "    \n",
    "            encoding = processor(\n",
    "                image, \n",
    "                words,\n",
    "                boxes=boxes,\n",
    "                max_length=512,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output = model(\n",
    "            input_ids=encoding[\"input_ids\"].to(DEVICE),\n",
    "            attention_mask=encoding[\"attention_mask\"].to(DEVICE),\n",
    "            bbox=encoding[\"bbox\"].to(DEVICE),\n",
    "            pixel_values=encoding[\"pixel_values\"].to(DEVICE)\n",
    "        )\n",
    "\n",
    "    predicted_class = output.logits.argmax()\n",
    "    return model.config.id2label[predicted_class.item()]\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "538a6767",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tahir iltaf\\.conda\\envs\\layoutlmv3_env\\lib\\site-packages\\transformers\\models\\layoutlmv3\\feature_extraction_layoutlmv3.py:30: FutureWarning: The class LayoutLMv3FeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use LayoutLMv3ImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = LayoutLMv3FeatureExtractor(apply_ocr=False)\n",
    "tokenizer = LayoutLMv3TokenizerFast.from_pretrained(\"microsoft/layoutlmv3-base\")\n",
    "processor = LayoutLMv3Processor(feature_extractor, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81a1d05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_path = Path('images/email/doc_000464.png')\n",
    "# prediction = predict_document_image2(image_path, model, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1f7f62f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('images/email/doc_000042.png'),\n",
       " WindowsPath('images/email/doc_000046.png'),\n",
       " WindowsPath('images/email/doc_000076.png'),\n",
       " WindowsPath('images/email/doc_000079.png'),\n",
       " WindowsPath('images/email/doc_000111.png'),\n",
       " WindowsPath('images/email/doc_000115.png'),\n",
       " WindowsPath('images/email/doc_000133.png'),\n",
       " WindowsPath('images/email/doc_000142.png'),\n",
       " WindowsPath('images/email/doc_000148.png'),\n",
       " WindowsPath('images/email/doc_000165.png'),\n",
       " WindowsPath('images/email/doc_000195.png'),\n",
       " WindowsPath('images/email/doc_000196.png'),\n",
       " WindowsPath('images/email/doc_000238.png'),\n",
       " WindowsPath('images/email/doc_000255.png'),\n",
       " WindowsPath('images/email/doc_000260.png'),\n",
       " WindowsPath('images/email/doc_000275.png'),\n",
       " WindowsPath('images/email/doc_000278.png'),\n",
       " WindowsPath('images/email/doc_000279.png'),\n",
       " WindowsPath('images/email/doc_000282.png'),\n",
       " WindowsPath('images/email/doc_000297.png'),\n",
       " WindowsPath('images/email/doc_000333.png'),\n",
       " WindowsPath('images/email/doc_000347.png'),\n",
       " WindowsPath('images/email/doc_000363.png'),\n",
       " WindowsPath('images/email/doc_000448.png'),\n",
       " WindowsPath('images/email/doc_000464.png'),\n",
       " WindowsPath('images/email/doc_000465.png'),\n",
       " WindowsPath('images/email/doc_000471.png'),\n",
       " WindowsPath('images/email/doc_000483.png'),\n",
       " WindowsPath('images/email/doc_000485.png'),\n",
       " WindowsPath('images/email/doc_000507.png'),\n",
       " WindowsPath('images/email/doc_000511.png'),\n",
       " WindowsPath('images/email/doc_000528.png'),\n",
       " WindowsPath('images/email/doc_000532.png'),\n",
       " WindowsPath('images/email/doc_000550.png'),\n",
       " WindowsPath('images/email/doc_000558.png'),\n",
       " WindowsPath('images/email/doc_000577.png'),\n",
       " WindowsPath('images/email/doc_000586.png'),\n",
       " WindowsPath('images/email/doc_000591.png'),\n",
       " WindowsPath('images/email/doc_000596.png'),\n",
       " WindowsPath('images/email/doc_000612.png'),\n",
       " WindowsPath('images/email/doc_000637.png'),\n",
       " WindowsPath('images/email/doc_000650.png'),\n",
       " WindowsPath('images/email/doc_000655.png'),\n",
       " WindowsPath('images/email/doc_000675.png'),\n",
       " WindowsPath('images/email/doc_000694.png'),\n",
       " WindowsPath('images/email/doc_000745.png'),\n",
       " WindowsPath('images/email/doc_000750.png'),\n",
       " WindowsPath('images/email/doc_000784.png'),\n",
       " WindowsPath('images/email/doc_000787.png'),\n",
       " WindowsPath('images/email/doc_000796.png'),\n",
       " WindowsPath('images/email/doc_000825.png'),\n",
       " WindowsPath('images/email/doc_000840.png'),\n",
       " WindowsPath('images/email/doc_000862.png'),\n",
       " WindowsPath('images/email/doc_000872.png'),\n",
       " WindowsPath('images/email/doc_000873.png'),\n",
       " WindowsPath('images/resume/doc_000051.png'),\n",
       " WindowsPath('images/resume/doc_000070.png'),\n",
       " WindowsPath('images/resume/doc_000072.png'),\n",
       " WindowsPath('images/resume/doc_000080.png'),\n",
       " WindowsPath('images/resume/doc_000088.png'),\n",
       " WindowsPath('images/resume/doc_000091.png'),\n",
       " WindowsPath('images/resume/doc_000097.png'),\n",
       " WindowsPath('images/resume/doc_000101.png'),\n",
       " WindowsPath('images/resume/doc_000109.png'),\n",
       " WindowsPath('images/resume/doc_000169.png'),\n",
       " WindowsPath('images/resume/doc_000173.png'),\n",
       " WindowsPath('images/resume/doc_000175.png'),\n",
       " WindowsPath('images/resume/doc_000191.png'),\n",
       " WindowsPath('images/resume/doc_000223.png'),\n",
       " WindowsPath('images/resume/doc_000248.png'),\n",
       " WindowsPath('images/resume/doc_000264.png'),\n",
       " WindowsPath('images/resume/doc_000281.png'),\n",
       " WindowsPath('images/resume/doc_000286.png'),\n",
       " WindowsPath('images/resume/doc_000294.png'),\n",
       " WindowsPath('images/resume/doc_000301.png'),\n",
       " WindowsPath('images/resume/doc_000344.png'),\n",
       " WindowsPath('images/resume/doc_000353.png'),\n",
       " WindowsPath('images/resume/doc_000361.png'),\n",
       " WindowsPath('images/resume/doc_000369.png'),\n",
       " WindowsPath('images/resume/doc_000375.png'),\n",
       " WindowsPath('images/resume/doc_000377.png'),\n",
       " WindowsPath('images/resume/doc_000402.png'),\n",
       " WindowsPath('images/resume/doc_000411.png'),\n",
       " WindowsPath('images/resume/doc_000441.png'),\n",
       " WindowsPath('images/resume/doc_000443.png'),\n",
       " WindowsPath('images/resume/doc_000447.png'),\n",
       " WindowsPath('images/resume/doc_000450.png'),\n",
       " WindowsPath('images/resume/doc_000460.png'),\n",
       " WindowsPath('images/resume/doc_000468.png'),\n",
       " WindowsPath('images/resume/doc_000473.png'),\n",
       " WindowsPath('images/resume/doc_000476.png'),\n",
       " WindowsPath('images/resume/doc_000499.png'),\n",
       " WindowsPath('images/resume/doc_000501.png'),\n",
       " WindowsPath('images/resume/doc_000543.png'),\n",
       " WindowsPath('images/resume/doc_000551.png'),\n",
       " WindowsPath('images/resume/doc_000575.png'),\n",
       " WindowsPath('images/resume/doc_000609.png'),\n",
       " WindowsPath('images/resume/doc_000629.png'),\n",
       " WindowsPath('images/resume/doc_000636.png'),\n",
       " WindowsPath('images/resume/doc_000639.png'),\n",
       " WindowsPath('images/resume/doc_000674.png'),\n",
       " WindowsPath('images/resume/doc_000727.png'),\n",
       " WindowsPath('images/resume/doc_000734.png'),\n",
       " WindowsPath('images/resume/doc_000752.png'),\n",
       " WindowsPath('images/resume/doc_000760.png'),\n",
       " WindowsPath('images/resume/doc_000763.png'),\n",
       " WindowsPath('images/resume/doc_000802.png'),\n",
       " WindowsPath('images/resume/doc_000809.png'),\n",
       " WindowsPath('images/resume/doc_000824.png'),\n",
       " WindowsPath('images/resume/doc_000847.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000016.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000045.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000050.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000061.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000089.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000121.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000128.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000130.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000171.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000228.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000235.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000240.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000250.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000285.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000298.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000309.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000311.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000334.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000345.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000349.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000392.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000429.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000437.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000438.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000445.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000451.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000453.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000480.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000498.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000509.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000513.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000515.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000522.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000530.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000534.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000584.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000594.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000615.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000617.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000697.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000698.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000741.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000756.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000766.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000767.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000769.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000779.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000785.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000821.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000832.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000845.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000864.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000891.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000942.png'),\n",
       " WindowsPath('images/scientific_publication/doc_000950.png')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_paths = sorted(list(Path(\"images\").glob(\"*/*.png\")))\n",
    "image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fe684c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'width_scale' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m ocr_result \u001b[38;5;241m=\u001b[39m reader\u001b[38;5;241m.\u001b[39mreadtext(\u001b[38;5;28mstr\u001b[39m(image_path))    \n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bbox, word, confidence \u001b[38;5;129;01min\u001b[39;00m ocr_result:\n\u001b[1;32m----> 6\u001b[0m     boxes\u001b[38;5;241m.\u001b[39mappend(scale_bounding_box(create_bounding_box(bbox), \u001b[43mwidth_scale\u001b[49m, height_scale))\n\u001b[0;32m      7\u001b[0m     words\u001b[38;5;241m.\u001b[39mappend(word)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'width_scale' is not defined"
     ]
    }
   ],
   "source": [
    "image_path = Path('images/email/doc_000464.png')\n",
    "words = []\n",
    "boxes = []\n",
    "ocr_result = reader.readtext(str(image_path))    \n",
    "for bbox, word, confidence in ocr_result:\n",
    "    boxes.append(scale_bounding_box(create_bounding_box(bbox), width_scale, height_scale))\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4211ca7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_paths = sorted(list(Path(\"images\").glob(\"*/*.png\")))\n",
    "# test_images = image_paths\n",
    "# labels = []\n",
    "# predictions = []\n",
    "# for image_path in tqdm(test_images):\n",
    "#     labels.append(image_path.parent.name)\n",
    "#     predictions.append(predict_document_image(image_path, model, processor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe14316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm = confusion_matrix(labels, predictions, labels=DOCUMENT_CLASSES)\n",
    "# cm_display = ConfusionMatrixDisplay(\n",
    "#     confusion_matrix=cm,\n",
    "#     display_labels=DOCUMENT_CLASSES\n",
    "# )\n",
    "\n",
    "# cm_display.plot()\n",
    "# cm_display.ax_.set_xticklabels(DOCUMENT_CLASSES, rotation=45)\n",
    "# cm_display.figure_.set_size_inches(16, 8)\n",
    "\n",
    "# plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
