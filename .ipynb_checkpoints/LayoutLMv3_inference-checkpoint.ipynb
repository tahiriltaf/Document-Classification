{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "65d13fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from pathlib import Path\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from transformers import LayoutLMv3FeatureExtractor, LayoutLMv3TokenizerFast, LayoutLMv3Processor, LayoutLMv3ForSequenceClassification\n",
    "from torchmetrics import Accuracy\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "420f48b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['email', 'resume', 'scientific_publication']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOCUMENT_CLASSES = ['email', 'resume', 'scientific_publication']\n",
    "DOCUMENT_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd5cb959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import nn\n",
    "\n",
    "# class ModelModule(nn.Module):\n",
    "class ModelModule(pl.LightningModule):\n",
    "    def __init__(self, n_classes:int):\n",
    "        super().__init__()\n",
    "        self.model = LayoutLMv3ForSequenceClassification.from_pretrained(\n",
    "            \"microsoft/layoutlmv3-base\", \n",
    "            num_labels=n_classes\n",
    "        )\n",
    "        self.model.config.id2label = {k: v for k, v in enumerate(DOCUMENT_CLASSES)}\n",
    "        self.model.config.label2id = {v: k for k, v in enumerate(DOCUMENT_CLASSES)}\n",
    "        self.train_accuracy = Accuracy(task=\"multiclass\", num_classes=n_classes)\n",
    "        self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, bbox, pixel_values, labels=None):\n",
    "        return self.model(\n",
    "            input_ids, \n",
    "            attention_mask=attention_mask,\n",
    "            bbox=bbox,\n",
    "            pixel_values=pixel_values,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        bbox = batch[\"bbox\"]\n",
    "        pixel_values = batch[\"pixel_values\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        output = self(input_ids, attention_mask, bbox, pixel_values, labels)\n",
    "        self.log(\"train_loss\", output.loss)\n",
    "        self.log(\"train_acc\", self.train_accuracy(output.logits, labels), on_step=True, on_epoch=True)\n",
    "        return output.loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        bbox = batch[\"bbox\"]\n",
    "        pixel_values = batch[\"pixel_values\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        output = self(input_ids, attention_mask, bbox, pixel_values, labels)\n",
    "        self.log(\"val_loss\", output.loss)\n",
    "        self.log(\"val_acc\", self.val_accuracy(output.logits, labels), on_step=False, on_epoch=True)\n",
    "        return output.loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.00001) #1e-5\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ae9d56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForSequenceClassification were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Tahir iltaf\\.conda\\envs\\layoutlmv3_env\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:751: UserWarning: You passed `Trainer(accelerator='cpu', precision=16)` but native AMP is not supported on CPU. Using `precision='bf16'` instead.\n",
      "  rank_zero_warn(\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model_module = ModelModule(len(DOCUMENT_CLASSES))\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filename=\"{epoch}-{step}-{val_loss:.4f}\", save_last=True, save_top_k=3, monitor=\"val_loss\", mode=\"min\"\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    precision=16,\n",
    "    devices=1,\n",
    "    max_epochs=10,\n",
    "    callbacks=[\n",
    "        model_checkpoint\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "790b2817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForSequenceClassification were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_path  = \"G:/programming/Packagex_assignment/LayoutLMv3/models/version_4/checkpoints/last.ckpt\"\n",
    "trained_model = ModelModule.load_from_checkpoint(\n",
    "    checkpoint_path = model_path, \n",
    "    n_classes=len(DOCUMENT_CLASSES), \n",
    "    local_files_only=True\n",
    ")\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = trained_model.model.eval().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d61902f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_document_image(\n",
    "    image_path: Path, \n",
    "    model: LayoutLMv3ForSequenceClassification, \n",
    "    processor: LayoutLMv3Processor):\n",
    "\n",
    "    json_path = image_path.with_suffix(\".json\")\n",
    "    with json_path.open(\"r\") as f:\n",
    "        ocr_result = json.load(f)\n",
    "\n",
    "        with Image.open(image_path).convert(\"RGB\") as image:\n",
    "\n",
    "            width, height = image.size\n",
    "            width_scale = 1000 / width\n",
    "            height_scale = 1000 / height\n",
    "    \n",
    "            words = []\n",
    "            boxes = []\n",
    "            for row in ocr_result:\n",
    "                boxes.append(\n",
    "                    scale_bounding_box(\n",
    "                        row[\"bounding_box\"], \n",
    "                        width_scale, \n",
    "                        height_scale\n",
    "                    )\n",
    "                )\n",
    "                words.append(row[\"word\"])\n",
    "    \n",
    "            encoding = processor(\n",
    "                image, \n",
    "                words,\n",
    "                boxes=boxes,\n",
    "                max_length=512,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output = model(\n",
    "            input_ids=encoding[\"input_ids\"].to(DEVICE),\n",
    "            attention_mask=encoding[\"attention_mask\"].to(DEVICE),\n",
    "            bbox=encoding[\"bbox\"].to(DEVICE),\n",
    "            pixel_values=encoding[\"pixel_values\"].to(DEVICE)\n",
    "        )\n",
    "\n",
    "    predicted_class = output.logits.argmax()\n",
    "    return model.config.id2label[predicted_class.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ff9825e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tahir iltaf\\.conda\\envs\\layoutlmv3_env\\lib\\site-packages\\transformers\\models\\layoutlmv3\\feature_extraction_layoutlmv3.py:30: FutureWarning: The class LayoutLMv3FeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use LayoutLMv3ImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = LayoutLMv3FeatureExtractor(apply_ocr=False)\n",
    "tokenizer = LayoutLMv3TokenizerFast.from_pretrained(\"microsoft/layoutlmv3-base\")\n",
    "processor = LayoutLMv3Processor(feature_extractor, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "44c6a344",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/165 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_path \u001b[38;5;129;01min\u001b[39;00m tqdm(test_images):\n\u001b[0;32m      6\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(image_path\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m----> 7\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpredict_document_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[41], line 8\u001b[0m, in \u001b[0;36mpredict_document_image\u001b[1;34m(image_path, model, processor)\u001b[0m\n\u001b[0;32m      6\u001b[0m json_path \u001b[38;5;241m=\u001b[39m image_path\u001b[38;5;241m.\u001b[39mwith_suffix(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m json_path\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 8\u001b[0m     ocr_result \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mopen(image_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m image:\n\u001b[0;32m     12\u001b[0m         width, height \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39msize\n",
      "\u001b[1;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "image_paths = sorted(list(Path(\"images\").glob(\"*/*.png\")))\n",
    "test_images = image_paths\n",
    "labels = []\n",
    "predictions = []\n",
    "for image_path in tqdm(test_images):\n",
    "    labels.append(image_path.parent.name)\n",
    "    predictions.append(predict_document_image(image_path, model, processor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6977d046",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(labels, predictions, labels=DOCUMENT_CLASSES)\n",
    "cm_display = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm,\n",
    "    display_labels=DOCUMENT_CLASSES\n",
    ")\n",
    "\n",
    "cm_display.plot()\n",
    "cm_display.ax_.set_xticklabels(DOCUMENT_CLASSES, rotation=45)\n",
    "cm_display.figure_.set_size_inches(16, 8)\n",
    "\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
